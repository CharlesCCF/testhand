<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hand Gesture Recognition</title>
    <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
    <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
    <style>
        video, canvas {
            width: 640px;
            height: 480px;
        }
    </style>
</head>
<body>
    <h1>Recognize hand gestures using the MediaPipe HandGestureRecognizer task</h1>

    <section id="demos">
        <h2>Demo: Webcam continuous hand gesture detection</h2>
        <p>Use your hand to make gestures in front of the camera to get gesture classification. <br>Click <b>enable webcam</b> below and grant access to the webcam if prompted.</p>

        <div id="liveView" class="videoView">
            <button id="webcamButton" class="mdc-button mdc-button--raised">
                <span class="mdc-button__ripple"></span>
                <span class="mdc-button__label">ENABLE WEBCAM</span>
            </button>
            <div style="position: relative;">
                <video id="webcam" autoplay playsinline></video>
                <canvas class="output_canvas" id="output_canvas" width="640" height="480" style="position: absolute; left: 0px; top: 0px;"></canvas>
                <p id='gesture_output' class="output"></p>
            </div>
        </div>
    </section>

    <script type="module">
        import {
            GestureRecognizer,
            FilesetResolver,
        } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

        import {
            HAND_CONNECTIONS,
            drawConnectors,
            drawLandmarks,
        } from "https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4";

        let gestureRecognizer;
        let webcamRunning = false;
        const videoHeight = "360px";
        const videoWidth = "480px";

        const createGestureRecognizer = async () => {
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
            );
            gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath:
                        "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
                    delegate: "GPU"
                },
                runningMode: "VIDEO"
            });
        };
        createGestureRecognizer();

        const video = document.getElementById("webcam");
        const canvasElement = document.getElementById("output_canvas");
        const canvasCtx = canvasElement.getContext("2d");
        const gestureOutput = document.getElementById("gesture_output");

        function hasGetUserMedia() {
            return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
        }

        if (hasGetUserMedia()) {
            const enableWebcamButton = document.getElementById("webcamButton");
            enableWebcamButton.addEventListener("click", enableCam);
        } else {
            console.warn("getUserMedia() is not supported by your browser");
        }

        async function enableCam(event) {
            if (!gestureRecognizer) {
                alert("Please wait for gestureRecognizer to load");
                return;
            }

            if (webcamRunning === true) {
                webcamRunning = false;
                event.target.innerText = "ENABLE WEBCAM";
            } else {
                webcamRunning = true;
                event.target.innerText = "DISABLE WEBCAM";

                const constraints = { video: true };
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                video.addEventListener("loadeddata", predictWebcam);
            }
        }

        async function predictWebcam() {
            if (!webcamRunning) return;

            const nowInMs = Date.now();
            const results = await gestureRecognizer.recognizeForVideo(video, nowInMs);

            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            canvasCtx.drawImage(video, 0, 0, canvasElement.width, canvasElement.height);

            if (results.landmarks) {
                for (const landmarks of results.landmarks) {
                    drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, { color: "#00FF00", lineWidth: 5 });
                    drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 2 });
                }
            }

            canvasCtx.restore();

            if (results.gestures.length > 0) {
                gestureOutput.style.display = "block";
                gestureOutput.style.width = videoWidth;
                const categoryName = results.gestures[0][0].categoryName;
                const categoryScore = parseFloat(
                    results.gestures[0][0].score * 100
                ).toFixed(2);
                const handedness = results.handednesses[0][0].displayName;
                gestureOutput.innerText = `GestureRecognizer: ${categoryName}\n Confidence: ${categoryScore}%\n Handedness: ${handedness}`;
            } else {
                gestureOutput.style.display = "none";
            }

            if (webcamRunning) {
                window.requestAnimationFrame(predictWebcam);
            }
        }
    </script>
</body>
</html>
